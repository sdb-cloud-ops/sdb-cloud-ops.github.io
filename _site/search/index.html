<!DOCTYPE html>
<html lang="en" class="no-js">
  <!-- Copyright 2019-2021 Vanessa Sochat-->
  <head>
<meta charset="utf-8">
<!-- Copyright 2019-2021 Vanessa Sochat-->

<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="generator" content="Hugo 0.55.6" />

<META NAME="ROBOTS" CONTENT="INDEX, FOLLOW">

<link rel="alternate" type="application/rss&#43;xml" href="/docs/index.xml">

<link rel="shortcut icon" href="/sdb-cloud-ops.github.io/assets/favicons/favicon.ico" >
<link rel="apple-touch-icon" href="/sdb-cloud-ops.github.io/assets/favicons/apple-touch-icon-180x180.png" sizes="180x180">
<link rel="icon" type="image/png" href="/sdb-cloud-ops.github.io/assets/favicons/favicon-16x16.png" sizes="16x16">
<link rel="icon" type="image/png" href="/sdb-cloud-ops.github.io/assets/favicons/favicon-32x32.png" sizes="32x32">
<link rel="icon" type="image/png" href="/sdb-cloud-ops.github.io/assets/favicons/android-36x36.png" sizes="36x36">
<link rel="icon" type="image/png" href="/sdb-cloud-ops.github.io/assets/favicons/android-48x48.png" sizes="48x48">
<link rel="icon" type="image/png" href="/sdb-cloud-ops.github.io/assets/favicons/android-72x72.png" sizes="72x72">
<link rel="icon" type="image/png" href="/sdb-cloud-ops.github.io/assets/favicons/android-96x196.png" sizes="96x196">
<link rel="icon" type="image/png" href="/sdb-cloud-ops.github.io/assets/favicons/android-144x144.png" sizes="144x144">
<link rel="icon" type="image/png" href="/sdb-cloud-ops.github.io/assets/favicons/android-192x192.png"sizes="192x192">

<title>Search</title>
<meta property="og:title" content="Search" />
<meta property="og:description" content="" />
<meta property="og:type" content="website" />
<meta property="og:url" content="http://localhost:4000" />
<meta property="og:site_name" content="http://localhost:4000" />

<meta itemprop="name" content="Search">
<meta itemprop="description" content="">

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Search"/>
<meta name="twitter:description" content=""/>

<link rel="stylesheet" href="/sdb-cloud-ops.github.io/assets/css/main.css">
<link rel="stylesheet" href="/sdb-cloud-ops.github.io/assets/css/palette.css">
<script
  src="/sdb-cloud-ops.github.io/assets/js/jquery-3.3.1/jquery-3.3.1.min.js"
  integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8="
  crossorigin="anonymous"></script>
</head>

  

  <body class="td-section">
    <header>
        <nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-column flex-md-row td-navbar">
        <a class="navbar-brand" href="/sdb-cloud-ops.github.io/">
            <span class="navbar-logo"></span><svg width="50" height="412" viewBox="0 0 463 412" fill="none" xmlns="http://www.w3.org/2000/svg">
<path d="M10.6163 388.655C7.52386 388.167 4.73368 387.446 2.19927 386.539V375.797C8.19815 377.75 13.8715 378.704 19.2426 378.704C23.9162 378.704 27.3806 377.797 29.6825 375.983C31.9612 374.169 33.1005 371.542 33.1005 368.054C33.1005 366.101 32.7517 364.474 32.0542 363.125C31.3566 361.776 30.1941 360.591 28.5665 359.544C26.9389 358.498 24.73 357.498 21.8933 356.545L17.336 355.01C6.19852 351.22 0.618164 344.152 0.618164 333.851C0.618164 329.248 1.61798 325.295 3.64086 321.993C5.66374 318.691 8.66318 316.157 12.6624 314.39C16.6617 312.646 21.6143 311.762 27.4736 311.762C30.1243 311.762 32.775 311.925 35.3791 312.274C37.9833 312.623 40.2852 313.088 42.2616 313.716V324.411C38.1461 323.109 33.5655 322.458 28.5432 322.458C18.3823 322.458 13.2902 325.946 13.2902 332.898C13.2902 334.781 13.6157 336.339 14.2435 337.595C14.8713 338.85 15.9409 339.99 17.4057 340.966C18.8938 341.966 20.94 342.896 23.5674 343.78L28.1247 345.314C33.9143 347.267 38.3088 349.965 41.3083 353.429C44.2845 356.894 45.7958 361.404 45.7958 366.961C45.7958 371.635 44.7728 375.658 42.7034 379.006C40.634 382.354 37.6345 384.935 33.6818 386.725C29.729 388.516 24.9625 389.399 19.4054 389.399C16.6384 389.399 13.7088 389.167 10.6163 388.655Z" fill="#1B1A21"/>
<path d="M54.2357 325.156C53.0964 324.086 52.5383 322.575 52.5383 320.622C52.5383 318.669 53.0964 317.157 54.2357 316.064C55.375 314.995 57.0491 314.437 59.258 314.437C61.4669 314.437 63.1643 314.995 64.3036 316.088C65.4429 317.18 66.0242 318.692 66.0242 320.622C66.0242 322.575 65.4662 324.086 64.3268 325.156C63.1875 326.225 61.5134 326.76 59.258 326.76C57.0491 326.76 55.375 326.225 54.2357 325.156Z" fill="#1B1A21"/>
<path d="M65.9085 332.62H53.469V388.423H65.9085V332.62Z" fill="#1B1A21"/>
<path d="M76.9285 332.619H87.2521L88.1822 338.595H88.8565C90.6236 336.386 92.7162 334.689 95.1111 333.503C97.506 332.317 100.157 331.736 103.063 331.736C107.806 331.736 111.573 333.224 114.34 336.2C117.107 339.176 118.502 343.989 118.502 350.593V388.4H106.132V351.546C106.132 348.407 105.505 346.175 104.249 344.85C102.993 343.524 101.203 342.85 98.8779 342.85C97.1108 342.85 95.3204 343.292 93.5533 344.152C91.7862 345.012 90.3678 346.338 89.2983 348.128V388.446H76.9285V332.619Z" fill="#1B1A21"/>
<path d="M143.987 402.002C148.73 402.002 152.218 400.909 154.473 398.723C156.729 396.538 157.845 392.864 157.845 387.679V382.773H157.24C156.054 384.726 154.404 386.26 152.334 387.423C150.265 388.586 147.893 389.167 145.219 389.167C141.871 389.167 138.825 388.237 136.058 386.377C133.291 384.517 131.059 381.517 129.385 377.402C127.688 373.286 126.851 368.008 126.851 361.591C126.851 351.639 129.083 344.175 133.524 339.246C137.965 334.293 144.708 331.805 153.753 331.782C156.543 331.782 159.426 332.015 162.402 332.503C165.378 332.991 167.982 333.619 170.238 334.456V385.656C170.238 394.887 168.145 401.606 164.006 405.768C159.844 409.93 153.52 412 145.033 412C144.591 412 144.429 412 143.987 411.977V402.002ZM154.52 377.657C155.938 376.96 157.054 375.983 157.868 374.774V342.036C156.171 341.664 154.45 341.478 152.753 341.478C148.358 341.478 145.056 342.92 142.848 345.826C140.639 348.733 139.546 353.592 139.546 360.451C139.546 365.218 139.964 368.961 140.825 371.635C141.685 374.309 142.871 376.169 144.382 377.192C145.894 378.215 147.823 378.727 150.149 378.727C151.637 378.727 153.102 378.378 154.52 377.657Z" fill="#1B1A21"/>
<path d="M181.282 388.446V305.764H193.652V388.423H181.282V388.446Z" fill="#1B1A21"/>
<path d="M244.154 364.497H214.95C215.09 368.287 215.648 371.24 216.648 373.403C217.648 375.542 219.159 377.076 221.205 378.006C223.251 378.936 225.972 379.378 229.39 379.378C232.901 379.378 236.97 378.727 241.573 377.425V387.493C239.085 388.167 236.76 388.679 234.598 388.981C232.436 389.283 230.203 389.446 227.925 389.446C221.996 389.446 217.159 388.469 213.416 386.493C209.672 384.517 206.905 381.424 205.092 377.193C203.278 372.937 202.371 367.334 202.371 360.335C202.371 350.779 204.255 343.641 208.021 338.874C211.788 334.107 217.113 331.736 224.018 331.736C237.458 331.736 244.178 341.385 244.178 360.684V364.497H244.154ZM219.159 341.687C217.88 342.641 216.857 344.315 216.113 346.71C215.369 349.105 214.974 352.383 214.927 356.592H232.482C232.459 352.406 232.087 349.105 231.412 346.71C230.738 344.315 229.785 342.641 228.553 341.687C227.32 340.734 225.786 340.246 223.949 340.246C222.042 340.246 220.438 340.734 219.159 341.687Z" fill="#1B1A21"/>
<path d="M260.659 388.655C257.566 388.167 254.776 387.446 252.242 386.54V375.797C258.24 377.75 263.914 378.704 269.285 378.704C273.958 378.704 277.423 377.797 279.725 375.983C282.003 374.17 283.143 371.542 283.143 368.055C283.143 366.101 282.794 364.474 282.096 363.125C281.399 361.777 280.236 360.591 278.609 359.545C276.981 358.498 274.772 357.498 271.935 356.545L267.378 355.011C256.217 351.197 250.66 344.129 250.66 333.828C250.66 329.225 251.66 325.272 253.683 321.97C255.706 318.668 258.705 316.134 262.705 314.367C266.704 312.623 271.656 311.74 277.516 311.74C280.167 311.74 282.817 311.902 285.421 312.251C288.026 312.6 290.327 313.065 292.304 313.693V324.388C288.188 323.086 283.608 322.435 278.585 322.435C268.425 322.435 263.332 325.923 263.332 332.875C263.332 334.758 263.658 336.316 264.286 337.572C264.914 338.827 265.983 339.967 267.448 340.943C268.936 341.943 270.982 342.873 273.61 343.757L278.167 345.291C283.957 347.245 288.351 349.942 291.351 353.406C294.327 356.871 295.838 361.381 295.838 366.939C295.838 371.612 294.815 375.635 292.746 378.983C290.676 382.331 287.677 384.912 283.724 386.702C279.771 388.493 275.005 389.376 269.448 389.376C266.657 389.399 263.728 389.167 260.659 388.655Z" fill="#1B1A21"/>
<path d="M332.25 378.1V388.423C329.553 389.074 326.762 389.4 323.856 389.4C318.02 389.4 313.602 387.865 310.556 384.773C307.51 381.68 305.999 376.774 305.999 370.078V342.501H297.466V332.62H305.999L308.208 312.716H318.392V332.62H331.785V342.501H318.392V368.59C318.392 371.147 318.671 373.17 319.252 374.635C319.834 376.1 320.74 377.146 321.973 377.774C323.205 378.402 324.856 378.727 326.925 378.727C328.181 378.727 329.948 378.518 332.25 378.1Z" fill="#1B1A21"/>
<path d="M341.643 382.47C337.784 377.82 335.854 370.566 335.854 360.753C335.854 341.408 343.317 331.759 358.222 331.759C365.383 331.759 370.894 334.13 374.777 338.874C378.637 343.617 380.59 350.872 380.59 360.66C380.59 379.866 373.126 389.469 358.245 389.469C351.037 389.469 345.503 387.121 341.643 382.47ZM363.523 378.355C364.895 377.262 365.964 375.309 366.732 372.519C367.499 369.728 367.848 365.845 367.848 360.846C367.848 355.731 367.476 351.732 366.732 348.872C365.988 346.012 364.918 344.012 363.5 342.896C362.081 341.78 360.338 341.199 358.222 341.199C356.083 341.199 354.292 341.757 352.897 342.873C351.502 343.989 350.432 345.942 349.688 348.779C348.944 351.616 348.572 355.522 348.572 360.544C348.572 365.636 348.944 369.589 349.688 372.402C350.432 375.239 351.502 377.192 352.897 378.308C354.269 379.424 356.059 379.982 358.245 379.982C360.384 380.006 362.128 379.448 363.523 378.355Z" fill="#1B1A21"/>
<path d="M389.215 332.619H399.446L400.423 340.292H401.027C402.399 337.432 404.236 335.316 406.584 333.968C408.909 332.619 411.56 331.945 414.536 331.945C415.722 331.945 416.908 332.038 418.07 332.247V343.803C416.908 343.594 415.49 343.501 413.769 343.501C411.281 343.501 408.933 344.082 406.7 345.221C404.492 346.361 402.771 347.942 401.585 349.965V388.423H389.215V332.619Z" fill="#1B1A21"/>
<path d="M462.831 364.497H433.627C433.766 368.287 434.324 371.24 435.324 373.403C436.324 375.542 437.835 377.076 439.882 378.006C441.928 378.936 444.648 379.378 448.066 379.378C451.577 379.378 455.646 378.727 460.25 377.425V387.493C457.762 388.167 455.437 388.679 453.274 388.981C451.112 389.283 448.88 389.446 446.601 389.446C440.672 389.446 435.836 388.469 432.092 386.493C428.349 384.517 425.582 381.424 423.768 377.193C421.955 372.937 421.048 367.334 421.048 360.335C421.048 350.779 422.931 343.641 426.698 338.874C430.465 334.107 435.789 331.736 442.695 331.736C456.134 331.736 462.854 341.385 462.854 360.684V364.497H462.831ZM437.835 341.687C436.557 342.641 435.534 344.315 434.789 346.71C434.045 349.105 433.65 352.383 433.604 356.592H451.159C451.135 352.406 450.763 349.105 450.089 346.71C449.415 344.315 448.461 342.641 447.229 341.687C445.997 340.734 444.462 340.246 442.625 340.246C440.719 340.246 439.114 340.734 437.835 341.687Z" fill="#1B1A21"/>
<path d="M237.644 0C265.606 7.77682 291.082 27.3744 301.645 51.327C317.8 90.2111 314.072 139.361 296.363 168.601C281.45 192.554 258.46 205.308 232.052 204.997C191.042 204.686 157.799 172.023 157.488 130.962C157.488 89.9 189.799 55.9931 232.052 55.9931C238.887 55.9931 248.742 56.941 259.927 61.6071C259.927 61.6071 251.886 56.8445 231.235 53.6453C171.895 45.8684 101.876 90.8332 118.031 194.42C140.4 234.238 183.275 261.301 232.052 260.99C303.82 260.679 362.539 201.886 362.228 129.717C362.228 60.6592 305.994 1.86644 237.644 0Z" fill="url(#paint0_linear)"/>
<path d="M286.421 55.0598C276.479 34.218 255.974 17.7311 230.808 11.1986C225.216 9.64319 219.313 9.02103 212.789 8.70996C203.158 8.70996 193.837 9.95429 184.206 12.4429C160.283 19.5975 143.506 33.5958 133.565 44.4833C120.827 59.1037 112.438 74.3463 107.467 89.5889C107.467 89.8999 107.157 90.211 107.157 90.8332C106.846 92.0775 105.603 95.8103 105.603 96.7436C105.292 97.3657 105.292 98.2989 104.982 98.9211C104.671 100.165 104.36 101.41 104.05 102.654C104.05 102.965 104.05 103.276 103.739 103.587C95.6612 143.716 109.08 179.727 117.158 193.103C119.179 196.449 120.838 199.24 122.5 201.801C104.17 102.258 156.866 54.7488 216.828 53.5045C248.207 52.8823 279.585 65.6363 294.809 91.1442C293.877 77.146 292.945 69.3692 286.421 55.0598Z" fill="url(#paint1_linear)"/>
<defs>
<linearGradient id="paint0_linear" x1="317.189" y1="-4.75735e-06" x2="126.668" y2="167.726" gradientUnits="userSpaceOnUse">
<stop stop-color="#FF00FF"/>
<stop offset="0.403975" stop-color="#AA00FF"/>
<stop offset="1" stop-color="#1E0A78"/>
</linearGradient>
<linearGradient id="paint1_linear" x1="198.026" y1="-33.8062" x2="313.821" y2="83.7228" gradientUnits="userSpaceOnUse">
<stop offset="0.0354358" stop-color="#FF00FF"/>
<stop offset="0.512854" stop-color="#8800CC"/>
<stop offset="1" stop-color="#311B92"/>
</linearGradient>
</defs>
</svg>
<span class="text-uppercase font-weight-bold">Cloud-ops</span>
	      </a>

	<div class="td-navbar-nav-scroll ml-md-auto center" id="main_navbar">
		<ul class="navbar-nav mt-2 mt-lg-0">
      <!--
			<li class="nav-item mr-4 mb-2 mb-lg-0">
                            <a class="nav-link" href="https://github.com/vsoch/docsy-jekyll" target="_blank"><span>GitHub</span></a>
			</li>
      -->
      
		</ul>
	</div>

	<div class="navbar-nav d-none d-lg-block">
    <input hidden type="search" class="form-control td-search-input" placeholder="&#xf002 Search this site…" aria-label="Search this site…" autocomplete="off">
  </div>

	<div class="navbar-nav d-none d-lg-block">
    <a class="gh-source" data-gh-source="github" href="https://github.com/vsoch/docsy-jekyll" title="Go to repository" data-md-state="done">
      <div hidden class="gh-source__repository">
        <i class="fab fa fa-github fa-2x" style='padding-right:20px; float:left; margin-top:5px'></i>
            vsoch/docsy-jekyll
        <ul class="gh-source__facts"><li class="gh-source__fact" id='stars'></li><li id="forks" class="gh-source__fact"></li></ul>
      </div>
    </a>
  </div>

</nav>
</header>

<script>
$(document).ready(function() {
  var url = "https://api.github.com/search/repositories?q=vsoch/docsy-jekyll";
  fetch(url, {
      headers: {"Accept":"application/vnd.github.preview"}
  }).then(function(e) {
    return e.json()
  }).then(function(r) {
     console.log(r.items[0])
     stars = r.items[0]['stargazers_count']
     forks = r.items[0]['forks_count']
     $('#stars').text(stars + " Stars")
     $('#forks').text(forks + " Forks")
  });
});
</script>

    <div class="container-fluid td-outer">
      <div class="td-main">
        <div class="row flex-xl-nowrap">
          <div class="col-12 col-md-3 col-xl-2 td-sidebar d-print-none">
          <div id="td-sidebar-menu" class="td-sidebar__inner">

  <form class="td-sidebar__search d-flex align-items-center">
    <input type="search" class="form-control td-search-input" placeholder="&#xf002 Search this site…" aria-label="Search this site…" autocomplete="off">
      <button class="btn btn-link td-sidebar__toggle d-md-none p-0 ml-3 fas fa-bars" type="button" data-toggle="collapse" data-target="#td-section-nav" aria-controls="td-docs-nav" aria-expanded="false" aria-label="Toggle section navigation">
      </button>
  </form>

  <nav class="collapse td-sidebar-nav pt-2 pl-4" id="td-section-nav">
    
      <ul class="td-sidebar-nav__section pr-md-3">
        
        

        <li class="td-sidebar-nav__section-title">
          <a  href="/sdb-cloud-ops.github.io/docs" class="align-left pl-0 pr-2 active td-sidebar-link td-sidebar-link__section">Technical Guides</a>
        </li>
        
          <ul>
            <li class="collapse show" id="technical-guides">
              <ul class="td-sidebar-nav__section pr-md-3">
                
                  <li class="td-sidebar-nav__section-title">
                    <a href="
                                /sdb-cloud-ops.github.io/docs/Deploying/deploying_home
                             "
                       class="align-left pl-0 pr-2 td-sidebar-link td-sidebar-link__section">Deploying
                    </a>
                  </li>

                  
                  

                  

                  
                
                  <li class="td-sidebar-nav__section-title">
                    <a href="
                                /sdb-cloud-ops.github.io/docs/Monitoring/monitoring
                             "
                       class="align-left pl-0 pr-2 td-sidebar-link td-sidebar-link__section">Monitoring
                    </a>
                  </li>

                  
                  

                  

                  
                
                  <li class="td-sidebar-nav__section-title">
                    <a href="
                                /sdb-cloud-ops.github.io/docs/Scaling/scaling
                             "
                       class="align-left pl-0 pr-2 td-sidebar-link td-sidebar-link__section">Scaling
                    </a>
                  </li>

                  
                  

                  

                  
                
              </ul>
            </li>
          </ul>
        
      </ul>
    
  </nav>
</div>

          </div>
          <div class="d-none d-xl-block col-xl-2 td-toc d-print-none">
              <div class="td-page-meta ml-2 pb-1 pt-2 mb-0 box0" hidden>
                  
<!--
<a href="https://github.com/vsoch/docsy-jekyll/edit/master/pages/search.html" target="_blank"><i class="fa fa-edit fa-fw"></i> Edit this page</a>
<a href="https://github.com/vsoch/docsy-jekyll/issues/new?labels=question&title=Question:&body=Question on: https://github.com/vsoch/docsy-jekyll/tree/master/pages/search.html" target="_blank"><i class="fab fa-github fa-fw"></i> Create documentation issue</a>
<a href="https://github.com/vsoch/docsy-jekyll/issues/new" target="_blank"><i class="fas fa-tasks fa-fw"></i> Create project issue</a>
-->

<a href="https://memsql.atlassian.net/secure/CreateIssueDetails!init.jspa?pid=11934&issuetype=10005&summary=Title%20goes%20here&description=Description%20goes%20here&priority=3" target="_blank"><i class="fab fa-jira fa-fw fa-lg"></i> Cloud-ops Jira Support </a>

<!-- this will parse through the header fields and add a button to open
     an issue / ask a question on Github. The editable field should be in
     the post frontend matter, and refer to the label to open the issue for -->

              </div>
              <nav id="TableOfContents"><ul>
              <li><ul id="TOC">
                <!-- Links will be appended here-->
              </ul></li>
              </ul></nav>
          </div>
          <main class="col-12 col-md-9 col-xl-8 pl-md-5" role="main">
            <nav aria-label="breadcrumb" class="d-none d-md-block d-print-none">
                <!--
        	      <ol class="breadcrumb spb-1">
                    
                    
                        
                            

                            <li class="breadcrumb-item active" aria-current="page">
                    	          <a href="http://localhost:4000/sdb-cloud-ops.github.io/search/">Search</a>
                            </li>

                            
                        
                    
        	      </ol>
                -->
                <ol class="breadcrumb spb-1">

                    

                    <!--<a href="/search/">/search/</a>-->

                    
                        
                        
                        

                        
                            <!------------------->
                            

                            <!-- the below variable is created specidifically to check /docs condition. the url is coming as /docs/ -->
                            

                            
                            <!--/search/^^^/docs^^^/docs/^^^Technical Guides-->

                            
                                <!--def-->
                                
                                    
                                        <!--****************-->
                                        
                                        
                                        <!--Deploying^^^/docs/Deploying/deploying_home-->

                                        
                                            <!--%%%%%%%%%%%%%%%%%%%%%-->
                                            
                                                <!--&&&&&&&&&&&&&&&&&&-->
                                                
                                                    
                                                    
                                                    <!--AWS!!!/docs/Deploying/AWS/deploying!!!/search/-->
                                                    
                                                        <!--%%%%%%%%%%%%%%%%%%%%%-->
                                                        <!--<a href="/sdb-cloud-ops.github.io/docs/Deploying/AWS/deploying">AWS</a>-->
                                                    
                                                
                                                    
                                                    
                                                    <!--GCP!!!/docs/Deploying/GCP/deploying!!!/search/-->
                                                    
                                                        <!--%%%%%%%%%%%%%%%%%%%%%-->
                                                        <!--<a href="/sdb-cloud-ops.github.io/docs/Deploying/GCP/deploying">GCP</a>-->
                                                    
                                                
                                                    
                                                    
                                                    <!--Azure!!!/docs/Deploying/Azure/deploying!!!/search/-->
                                                    
                                                        <!--%%%%%%%%%%%%%%%%%%%%%-->
                                                        <!--<a href="/sdb-cloud-ops.github.io/docs/Deploying/Azure/deploying">Azure</a>-->
                                                    
                                                
                                            

                                            <!--<a href=""></a>-->
                                        



                                        

                                    
                                        <!--****************-->
                                        
                                        
                                        <!--Monitoring^^^/docs/Monitoring/monitoring-->

                                        
                                            <!--%%%%%%%%%%%%%%%%%%%%%-->
                                            

                                            <!--<a href=""></a>-->
                                        



                                        

                                    
                                        <!--****************-->
                                        
                                        
                                        <!--Scaling^^^/docs/Scaling/scaling-->

                                        
                                            <!--%%%%%%%%%%%%%%%%%%%%%-->
                                            

                                            <!--<a href=""></a>-->
                                        



                                        

                                    
                                
                            

                            

                        
                        <!--###############-->
                        <!--Technical Guides^^^/docs-->
                        
                        <!--###############-->
                        <!--^^^Scaling^^^/docs/Scaling/scaling-->
                        

                        <!--###############-->
                        <!--Azure^^^/docs/Deploying/Azure/deploying-->
                        
                    

<!--
                    
                    
                        
                            
                            <li class="breadcrumb-item active" aria-current="page">
                    	          <a href="http://localhost:4000/sdb-cloud-ops.github.io/search/">Search</a>
                            </li>
                            
                        
                    
                  -->
        	      </ol>
            </nav>


           <div class="td-content">
	      <input class="form-control td-search-input" type="search" name="q" id="search-input" placeholder="&#xf002 Search this site…"  style="margin-top:5px" autofocus>
<i style="color:white; margin-right:8px; margin-left:5px" class="fa fa-search"></i>

<p><span id="search-process">Loading</span> results <span id="search-query-container" style="display: none;">for "<strong id="search-query"></strong>"</span></p>

<ul id="search-results"></ul>

<script>
	window.data = {
		
				
					
					
					"docs-deploying-aws-deploying": {
						"id": "docs-deploying-aws-deploying",
						"title": "Deploy SingleStore with Amazon Web Service",
						"categories": "",
						"url": " /docs/Deploying/AWS/deploying",
						"content": "Deploy SingleStore DB in Amazon Elastic Kubernetes Service (EKS)\n\nIntroduction\n\nUse these steps to deploy SingleStore DB in Amazon Elastic Kubernetes\nService (EKS)\n\nSummary\n\nSingleStore’s Cluster Management in kubernetes is different from native\ndeployment. All jobs related to cluster management are done by\noperators.\n\nReference:\nhttps://docs.singlestore.com/db/v7.3/en/reference/memsql-operator-reference/memsql-operator-reference-overview.html\n\nUser prohibited to use below command or tools:\n\n\n  \n    Cluster management command\n  \n  \n    SingleStore DB Toolbox except sdb-report\n\n    a.  sdb-toolbox-config. Performs host machine registration.\n\n    b.  sdb-deploy. Installs memsqlctl and the SingleStore DB database engine to host machines in the cluster.\n\n    c.  sdb-admin. Helps you manage a SingleStore DB cluster.\n\n    d.  sdb-report. Collects and performs diagnostic checks on your cluster.\n\n    e.  memsqlctl. Provides lower-level access to manage nodes on a host machine.\n  \n\n\nEKS Cluster\n\nPrerequisite\n\nRole\n\nCreate two roles\n\nCluster IAM Role\n\nReference:\nhttps://docs.aws.amazon.com/eks/latest/userguide/service_IAM_role.html\n\n\n  \n    Open the IAM console at https://console.aws.amazon.com/iam/\n  \n  \n    In the left navigation pane, choose Roles.\n  \n  \n    Search the list of roles for eksClusterRole.\nIf a role that includes eksClusterRole doesn’t exist, then continue the next step to create the role else skip the rest of steps.\n  \n  \n    Click Create role\n\n    \n  \n  \n    Select trusted entity\n  \n  \n    Select AWS service\n  \n  \n    Select EKS - Cluster as Use cases\n\n    \n  \n  \n    Add permissions\n\n    \n  \n  \n    Name, review, and create\n\n    \n  \n\n\nReference:\nhttps://docs.aws.amazon.com/eks/latest/userguide/security-iam-awsmanpol.html\n\nAssign all *EKS* managed policy to the role\n\n\n\nService IAM Role\n\n\n  \n    Open the IAM console at https://console.aws.amazon.com/iam/\n  \n  \n    In the left navigation pane, choose Roles.\n  \n  \n    Search the list of roles for AWSServiceRoleForAmazonEKSNodegroup.\nIf a role that includes AWSServiceRoleForAmazonEKSNodegroup doesn’t exist, then continue the next step to create the role else skip the rest of steps.\n  \n  \n    Click Create role\n\n    \n  \n  \n    Select trusted entity\n  \n  \n    Select AWS service\n  \n  \n    Select EKS - Nodegroup as Use cases\n\n    \n  \n  \n    Add permissions\n\n    \n  \n  \n    Name, review, and create\n\n    \n  \n\n\nNode IAM Role\n\nReference:\nhttps://docs.aws.amazon.com/eks/latest/userguide/create-node-role.html\n\n\n  \n    Open the IAM console at https://console.aws.amazon.com/iam/\n  \n  \n    In the left navigation pane, choose Roles.\n  \n  \n    Search the list of roles for AmazonEKSNodeRole.\n If a role that includes AWSServiceRoleForAmazonEKS doesn’t exist, then continue the next step to create the role else skip the rest of steps.\n  \n  \n    Click Create role\n\n    \n  \n  \n    Select trusted entity\n  \n  \n    Select AWS service\n  \n  \n    Select EC2 as Use cases\n\n    \n  \n  \n    Add permissions, select AmazonEKSWorkerNodePolicy and AmazonEC2ContainerRegistryReadOnly\n\n    \n\n    \n  \n  \n    Name, review, and create\n\n    \n  \n\n\nPolicy\n\n\n  \n    Open the IAM console at https://console.aws.amazon.com/iam/\n  \n  \n    In the left navigation pane, choose Policies.\n  \n  \n    Click Create Policy.\n\n    \n  \n  \n    Copy the following aws permission on JSON tab\n\n    \n  \n\n\n        {\n            \"Version\": \"2012-10-17\",\n            \"Statement\": [\n                {\n                    \"Sid\": \"VisualEditor0\",\n                    \"Effect\": \"Allow\",\n                    \"Action\": [\n                        \"iam:ListRoles\",\n                        \"ec2:DescribeSubnets\",\n                        \"eks:CreateCluster\"\n                    ],\n                    \"Resource\": \"*\"\n                },\n                {\n                    \"Sid\": \"VisualEditor1\",\n                    \"Effect\": \"Allow\",\n                    \"Action\": [\n                        \"eks:DeleteCluster\",\n                        \"iam:GetRole\",\n                        \"iam:PassRole\",\n                        \"iam:ListAttachedRolePolicies\",\n                        \"eks:DeleteNodegroup\",\n                        \"eks:TagResource\",\n                        \"eks:DescribeCluster\",\n                        \"eks:CreateNodegroup\"\n                    ],\n                    \"Resource\": [\n                        \"arn:aws:iam::[account-id]:role/AmazonEKSNodeRole\",\n                        \"arn:aws:iam::[account-id]:role/eksClusterRole\",\n                        \"arn:aws:iam::[account-id]:role/*AWSServiceRoleForAmazonEKSNodegroup\",\n                        \"arn:aws:eks:us-east-1:[account-id]:nodegroup/eks-cluster/*/*\",\n                        \"arn:aws:eks:us-east-1:[account-id]:cluster/eks-cluster\"\n                    ]\n                }\n            ]\n        }\n\n\n\n  \n    Review policy, put name eks-policy\n\n    \n  \n\n\nUser\n\nCreate user service\n\n\n  \n    Open the IAM console at https://console.aws.amazon.com/iam/\n  \n  \n    In the left navigation pane, choose Users.\n  \n  \n    Set user details\n  \n  \n    Select Access key - Programmatic access only\n\n    \n  \n  \n    Add permission\n  \n  \n    Choose Attach existing policies directly.\n\n    Attach eks-policy to users.\n\n    \n  \n\n\nCreate the Cluster\n\nReference:\nhttps://docs.aws.amazon.com/eks/latest/userguide/create-cluster.html#aws-cli\n\n\n  Define\n    region-code\ncluster-name\naccount_id\nsubnetId\n    \n  \n  \n    Create cluster\n\n    Use comma as delimiter if you have more than one subnet\n\n    Create your cluster with the following command:\n\n    aws eks create-cluster \\\n--region region-code \\\n--name cluster-name \\\n--kubernetes-version 1.21 \\\n--role-arn arn:aws:iam::\\[account-id\\]:role/eksClusterRole \\\n--resources-vpc-config subnetIds=subnetId1,subnetId2\n    \n\n    \n\n    This process may take several minutes.\n  \n\n\nAccess the Cluster\n\nConnect kubectl\n\naws eks update-kubeconfig --region us-east-1 --name eks-cluster\n\n\nVerify\n\nkubectl get node -o wide\n\n\n\n\nDelete VPC CNI\n\nReference:\nhttps://docs.cilium.io/en/v1.9/gettingstarted/k8s-install-eks/\n\nCilium will manage ENIs instead of VPC CNI, so the aws-node DaemonSet\nhas to be deleted to prevent conflict behavior.\n\nkubectl -n kube-system delete daemonset aws-node\n\n\n\n\nWait the cluster creation complete if you above result\n\n\n\nYou can continue to the next step using kubectl command\n\nDeploy Cilium as CNI\n\nReference:\nhttps://docs.cilium.io/en/v1.9/gettingstarted/k8s-install-eks/#deploy-cilium\n\nSetup Helm repository:\n\nhelm repo add cilium https://helm.cilium.io/\n\n\nThis helm command sets eni=true and tunnel=disabled, meaning the Cilium\nwill allocate a fully-routable AWS ENI IP address for each pod, similar\nto the behavior of the Amazon VPC CNI\nplugin.\n\nExcluding the lines for eni=true, ipam.mode=eni and tunnel=disabled from\nthe helm command will configure Cilium to use overlay routing mode.\nEnsure the pod CIDR (ipam.operator.clusterPoolIPv4PodCIDR) is not\noverlapping with node CIDR.\n\nDeploy Cilium release via Helm:\n\nhelm install cilium cilium/cilium --version 1.9.13 \\\n--namespace kube-system \\\n--set eni=true \\\n--set ipam.mode=eni \\\n--set egressMasqueradeInterfaces=eth0 \\\n--set tunnel=disabled \\\n--set nodeinit.enabled=true\n\n\nCreate the Node Group\n\nReference:\nhttps://docs.aws.amazon.com/eks/latest/userguide/create-cluster.html#aws-cli\n\n\n  \n    Define\nregion-code\ncluster-name\naccount_id\nsubnetId\n  \n  \n    Create node group\n\n    Use space as delimiter if you have more than one subnet\n\n    Create node group with the following command:\n        aws eks create-nodegroup \\\n    --cluster-name cluster-name \\\n    --region region-code \\\n    --nodegroup-name ng-1 \\\n    --scaling-config minSize=1,maxSize=2,desiredSize=1 \\\n    --instance-types t2.large \\\n    --subnets subnetId1 subnetId2 \\\n    --node-role arn:aws:iam::\\[accountId\\]:role/AmazonEKSNodeRole\n    \n    \n\n    This process may take several minutes.\n  \n\n\nVerification\n\n\n  \n    Ensure the node’s status is Ready and all pod’s status Running.\n\n    kubectl get nodes\n\nkubectl get po -A\n    \n\n    \n  \n  \n    In cilium operator logs show Initialization complete\n\n    kubectl logs \\[cilium-operator-pod-name\\] -nkube-system --tail 10\n    \n\n    \n  \n\n\nSingleStore\n\nCluster Admin Prerequisites\n\n\n  \n    Determine the project (namespace) in which to deploy SingleStore DB.\n  \n  \n    Determine which StorageClass (SC) to use.\n\n    Avoid using a StorageClass with an NFS-based provisioner. Ideally, you should choose a StorageClass that uses a block storage-based provisioner that supports volume expansion and the WaitForFirstConsumer binding mode.\n\n    Available SC in EKS:\n\n    \n  \n  \n    Determine the fsGroup to use for the deployment.\n  \n\n\nDeployment Prerequisites\n\n\n  \n    Obtain a SingleStore license from the SingleStore Customer Portal.\n  \n  \n    Select the SingleStore DB images to use. Two Docker images are required for the deployment.\n\n    a.  The node image is the SingleStore DB database engine and can be found on Docker Hub.\n\n    b.  The Operator image is used to manage the SingleStore DB engine deployment in Kubernetes environment, and can also be found on Docker Hub.\n  \n  \n    Use the StorageClass that you selected in Cluster Admin Prerequisites.\n  \n  \n    Substitute the fsGroup value with the value you copied in Cluster Admin Prerequisites.\n  \n\n\nCreate the Object Definition Files\n\nThe following are definition files that will be used by the Operator to\ncreate your cluster. Create new definition files and copy and paste the\ncontents of each code block into those files.\n\n\n  \n    deployment.yaml\n  \n  \n    rbac.yaml\n  \n  \n    memsql-cluster-crd.yaml\n  \n  \n    memsql-cluster.yaml\n  \n\n\nMemSQL Operator\n\nFor –cluster-id, enter the name of your cluster. This will be the name used in the memsql-cluster.yaml file as per:\n\nmetadata:\n    name: memsql-cluster\n\n\nCreate a deployment definition file using the template below.\n\ncat &gt; deployment.yaml &lt;&lt;EOF\napiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: memsql-operator\nspec:\nreplicas: 1\nselector:\nmatchLabels:\n  name: memsql-operator\ntemplate:\nmetadata:\n  labels:\n    name: memsql-operator\nspec:\n  serviceAccountName: memsql-operator\n  containers:\n    - name: memsql-operator\n      image: memsql/operator:1.2.5-83e8133a\n      imagePullPolicy: Always\n      args:\n        - \"--cores-per-unit\"\n        - \"8\"\n        - \"--memory-per-unit\"\n        - \"32\"\n        - \"--merge-service-annotations\"\n        - \"--cluster-id\"\n        - \"memsql-cluster\"\n        - \"--fs-group-id\"\n        - \"5555\"\n      env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: memsql-operator\n        - name: RELATED_IMAGE_BACKUP\n          value: memsql/tools\nEOF\n\n\nRBAC\n\nCopy the following to create a ServiceAccount definition file that will\nbe used by the MemSQL Operator.\n\ncat &gt; rbac.yaml &lt;&lt;EOF\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: memsql-operator\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: memsql-operator\nrules:\n- apiGroups:\n  - \"\"\n  resources:\n  - pods\n  - services\n  - endpoints\n  - persistentvolumeclaims\n  - events\n  - configmaps\n  - secrets\n  verbs:\n  - '*'\n- apiGroups:\n  - policy\n  resources:\n  - poddisruptionbudgets\n  verbs:\n  - '*'\n- apiGroups:\n  - batch\n  resources:\n  - cronjobs\n  verbs:\n  - '*'\n- apiGroups:\n  - \"\"\n  resources:\n  - namespaces\n  verbs:\n  - get\n- apiGroups:\n  - apps\n  - extensions\n  resources:\n  - deployments\n  - daemonsets\n  - replicasets\n  - statefulsets\n  - statefulsets/status\n  verbs:\n  - '*'\n- apiGroups:\n  - memsql.com\n  resources:\n  - '*'\n  verbs:\n  - '*'\n- apiGroups:\n  - networking.k8s.io\n  resources:\n  - networkpolicies\n  verbs:\n  - '*'\n- apiGroups:\n  - coordination.k8s.io\n  resources:\n  - leases\n  verbs:\n  - get\n  - list\n  - watch\n  - create\n  - update\n  - patch\n  - delete\n---\nkind: RoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: memsql-operator\nsubjects:\n- kind: ServiceAccount\n  name: memsql-operator\nroleRef:\n  kind: Role\n  name: memsql-operator\n  apiGroup: rbac.authorization.k8s.io\nEOF\n\n\nCustom Resource Definition\n\n\n  \n    Update apiVersion from v1alpha to v1\n  \n  \n    Restructure the spec\n  \n  \n    Create the CRD file use following command\n\n    cat &gt; memsql-cluster-crd.yaml &lt;&lt;EOF\napiVersion: apiextensions.k8s.io/v1\nkind: CustomResourceDefinition\nmetadata:\n  name: memsqlclusters.memsql.com\nspec:\n  group: memsql.com\n  names:\n    kind: MemsqlCluster\n    listKind: MemsqlClusterList\n    plural: memsqlclusters\n    singular: memsqlcluster\n    shortNames:\n      - memsql\n  scope: Namespaced\n  versions:\n    - name: v1alpha1\n      served: true\n      storage: true\n      schema:\n        openAPIV3Schema:\n          type: object\n          x-kubernetes-preserve-unknown-fields: true\n      additionalPrinterColumns:\n      - name: Aggregators\n        type: integer\n        description: Number of SingleStore DB Aggregators\n        jsonPath: .spec.aggregatorSpec.count\n      - name: Leaves\n        type: integer\n        description: Number of SingleStore DB Leaves (per availability group)\n        jsonPath: .spec.leafSpec.count\n      - name: Redundancy Level\n        type: integer\n        description: Redundancy level of SingleStore DB Cluster\n        jsonPath: .spec.redundancyLevel\n      - name: Age\n        type: date\n        jsonPath: .metadata.creationTimestamp\nEOF\n    \n  \n\n\nMemSQL Cluster\n\nCreate a MemSQLCluster definition file to specify the configuration settings for your cluster.\n\ncat &gt; memsql-cluster.yaml &lt;&lt;EOF\napiVersion: memsql.com/v1alpha1\nkind: MemsqlCluster\nmetadata:\n  name: memsql-cluster\nspec:\n  # TODO: paste your license key from https://portal.singlestore.com here:\n  license: REPLACE_THIS_WITH_LICENSE\n\n  # TODO: replace the default admin password for production environment\n  # select password(\"secret\");\n  adminHashedPassword: \"*14E65567ABDB5135D0CFD9A70B3032C179A49EE7\"\n  nodeImage:\n    repository: memsql/node\n    tag: latest\n\n  # TODO: set greater than 1 to enable HA mode\n  redundancyLevel: 1\n  serviceSpec:\n    objectMetaOverrides:\n      labels:\n        custom: label\n      annotations:\n        custom: annotations\n  aggregatorSpec:\n    count: 1\n    height: 0.25\n    storageGB: 20\n    storageClass: default\n    objectMetaOverrides:\n      annotations:\n        optional: annotation\n      labels:\n        optional: label\n  leafSpec:\n    count: 1\n    height: 0.25\n    storageGB: 20\n    storageClass: default\n    objectMetaOverrides:\n      annotations:\n        optional: annotation\n      labels:\n        optional: label\nEOF\n\n\nDeploy a SingleStore DB Cluster\n\nReference: Deploy a SingleStore DB Cluster\n\nNow that your various object definition files are created, you will use kubectl to do the actual object creation and cluster deployment.\n\n\n  \n    Install the RBAC resources.\n\n    kubectl create -f rbac.yaml -n&lt;namespace&gt;\n    \n  \n  \n    Install the MemSQL cluster resource definition.\n\n    kubectl create -f memsql-cluster-crd.yaml\n    \n  \n  \n    Deploy the Operator.\n\n    kubectl create -f deployment.yaml -n&lt;namespace&gt;\n    \n  \n  \n    Verify the deployment was successful by checking the status of the pods in your Kube cluster. You should see the Operator with a status of Running.\n\n    kubectl get pods\n    \n  \n  \n    Finally, create the cluster.\n\n    kubectl create -f memsql-cluster.yaml\n    \n  \n  \n    After a couple minutes, run kubectl get pods again to verify the aggregator and leaf nodes all started and have a status of Running.\n\n    kubectl get pods\n    \n\n    If you see no pods are in the Running state, check the Operator logs by running\n\n    kubectl logs deployment memsql-operator -n&lt;namespace&gt;\n    \n\n    Then look at the various objects to see what is failing.\n  \n\n\nVerification\n\n\n  \n    Verify the cluster\n\n    kubectl get memsqlcluster memsql-cluster -o=jsonpath='{.status.phase}{\"\\n\"}' -n&lt;name-space&gt;\n\nThe SingleStore DB server deployment is complete when Running is displayed after running the above commands.\n    \n  \n  \n    Verify the pod list, run the following command to display the pod list.\n\n    kubectl get po -n&lt;name-space&gt;\n\nResult may vary:\n    \n\n    \n  \n  \n    After the deployment completes, run the following command to display the two SingleStore DB service endpoints that are created during the deployment.\n\n    kubectl get svc | grep &lt;cluster-name&gt;\n    \n\n    The svc--ddl and svc--dml service endpoints can be used to connect to SingleStore DB using a MySQL compatible client. Note that svc--dml only exists if memsqlCluster.aggregatorSpec.count is greater than 1.\n\n    The output will resemble the following (actual values will vary):\n\n    NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE\nsvc-memsql-cluster ClusterIP None &lt;none&gt; 3306/TCP 44h\nsvc-memsql-cluster-ddl LoadBalancer 10.101.189.86 169.46.26.10\n3306:30351/TCP 44h\nsvc-memsql-cluster-dml LoadBalancer 10.103.29.220 169.46.26.11\n3306:32524/TCP 44h\nsvc-memsql-studio LoadBalancer 10.97.104.121 169.46.26.11 8081:31161/TCP\n43h\n    \n\n    Column definition:\n\n    NAME EXTERNAL-IP PORT(S) AGE\nService name External IP Service Port:Node Port Service created\n    \n\n    Refer to Data Definition Language DDL and Data Manipulation Language DML for more information.\n  \n\n\nInstall SingleStore Client\n\n\n  \n    Add the SingleStore repository to your repository list.\n\n    sudo yum-config-manager --add-repo https://release.memsql.com/production/rpm/x86_64/repodata/memsql.repo\n    \n  \n  \n    Verify that the SingleStore repo information is listed under repolist.\n\n    sudo yum repolist\n    \n  \n  \n    Verify that the which package is installed. This is used during the install process to identify the correct package type for your installation.\n\n    rpm -q which\n    \n  \n  \n    Install which\nSkip this step if you have it\n\n    sudo yum install -y which\n    \n  \n  \n    Install the SingleStore client.\n\n    sudo yum install -y singlestore-client\n    \n  \n\n\nAccess SingleStore DB\n\n\n  \n    Connect via Load Balancer / External IP (refer svc-memsql-cluster-ddl endpoint)\n\n    singlestore -u admin -h &lt;external-ip&gt; -p\n    \n\n    \n  \n  \n    Connect via Node Port (refer svc-memsql-cluster-ddl endpoint)\nIn case the External-IP is pending, we can access through Node Port Get the host IP of singlestore’s master node:\n\n    kubectl get po node-memsql-cluster-master-0 -n&lt;namespace&gt; -o=jsonpath='{.status.hostIP}{\"\\n\"}'\n    \n\n    Sample output: 172.31.21.171\n\n    singlestore -u admin -h &lt;host-ip&gt; -P &lt;node-port&gt; -p\n    \n\n    \n  \n  \n    Check status of aggregator and leaf, run the following command\n\n    show aggregators;\n\nshow leaves;\n    \n\n    The result may vary:\n\n    \n  \n\n\nRollback/Cleanup\n\nEKS Cluster\n\nSkip this step if you want to retain the EKS cluster for another\napplication\nNode Group\naws eks delete-nodegroup \\\n--cluster-name cluster-name \\\n--region region-code \\\n--nodegroup-name ng-1\n\nCluster\naws eks delete-cluster \\\n  --region region-code \\\n  --name cluster-name\n\nSingleStore DB\n\nThis step is no longer required if you did the previous step (delete EKS cluster).\n\n\n  Delete the cluster.\n    kubectl delete -f memsql-cluster.yaml -n&lt;namespace&gt;\n    \n  \n  Clean up the PersistentVolumeClaim\n    kubectl delete pvc --all -n&lt;namespace&gt;\n    \n  \n  Delete the Operator.\n    kubectl delete -f deployment.yaml -n&lt;namespace&gt;\n    \n  \n  Delete RBAC\n    kubectl create -f rbac.yaml -n&lt;namespace&gt;\n    \n  \n  Delete the MemSQL custom resource definition.\n    kubectl create -f memsql-cluster-crd.yaml\n    \n  \n  Clean up the Namespaces\n    kubectl delete ns &lt;namespaces&gt;\n    \n  \n\n\nUseful Command\n\nKubernetes\n\nKubernetes object\n\nstatefulsets, deployment, pod, memsqlcluster, pvc, pv\n\n\nkubectl command:\n\nGo to inside pod:\n\nkubectl exec -it \\[pod name\\] -n &lt;namespace&gt; -- bash\n\n\nOther:\n\nkubectl get statefulsets\n\nkubectl get deploy -n memsql\n\nkubectl describe deploy \\[deployment name\\] -n memsql\n\nkubectl get pod -n memsql\n\nkubectl describe pod \\[pod name\\] -n memsql\n\nkubectl get memsqlcluster -n memsql\n\nkubectl describe memsqlcluster \\[memsqlcluster name\\] -n memsql\n\nkubectl get pvc -n memsql\n\nkubectl describe pvc \\[pvc name\\] -n memsql\n\nkubectl get pv -n memsql\n\nkubectl describe pv \\[pv name\\] -n memsql\n\nkubectl logs \\[pod name\\] -n memsql\n\nkubectl logs -f \\[pod name\\] -n&lt;namespace&gt;\n\n\nSingleStore\n\nsinglestore command need to run inside the master pod:\n\nCheck license:\n\nmemsqlctl show-license\n\n\nList MemSQL Nodes on the local machine:\n\nmemsqlctl list-nodes\n\n\nList aggregator node:\n\nmemsqlctl show-aggregators\n\n\nList leaves node:\n\nmemsqlctl show-leaves"
					}
					
				
		
				
					,
					
					"docs-deploying-azure-deploying": {
						"id": "docs-deploying-azure-deploying",
						"title": "Deploy SingleStore with Azure Kubernetes Service",
						"categories": "",
						"url": " /docs/Deploying/Azure/deploying",
						"content": "Deploy SingleStore with Azure Kubernetes Service\n\nPrerequisites\n\nMake sure you have kubectl and Azure CLI installed\n\nObtain a SingleStore license from the SingleStore Customer Portal.\n\nUse the az login command to initially login to Azure\n\nCreating and connecting to the Kubernetes cluster\n\nWe will use the az aks create command to create our sdb-cluster using the following flags:\n\n--name and --resource-group are required\n\n--node-count\n--node-vm-size\n--enable-cluster-autoscaler\n--min-count\n--max-count\n--network-plugin\n--network-policy\n\n\nExample command\naz aks create --name sdb-cluster --resource-group singlestore --node-count 4 --node-vm-size standard_d4s_v3 --enable-cluster-autoscaler --min-count 1 --max-count 12 --network-plugin azure --network-policy calico\n\n\n  Azure CNI and Calico are required for SingleStore\n\n\nTo set your account\naz account set --subscription &lt;subscription-id&gt;\n\nTo get your credentials\nConfigure kubectl to connect to your Kubernetes cluster using the az aks get-credentials command. The following command downloads credentials and configures the Kubernetes CLI to use them.\n\naz aks get-credentials --resource-group singlestore --name sdb-cluster\n\nVerify\nIf you have kubectl correctly installed, you should now be able to run kubectl get ns without errors.\n\nDeploy SingleStore on Kubernetes\n\nDownload the Operator image (optional if not pulling from deployment.yaml)\n\nThe memsql/operator can be pulled from Docker Hub\n\nExample with Docker\ndocker pull memsql/operator:1.2.5-83e8133a\n\n\nCreate the Object Definition files\n\nDownload the object definition files from our site, and save them in an accessible directory. The rbac.yaml and the memsql-cluster-crd.yaml do not require edits.\n\nEdit the deployment.yaml and reference the image you downloaded, or you can pull it automatically.\n\ndeployment.yaml\n\nExample spec\n    spec:\n      serviceAccountName: memsql-operator\n      containers:\n        - name: memsql-operator\n          image: memsql/operator:1.2.5-83e8133a\n          imagePullPolicy: IfNotPresent\n\n\nmemsql-cluster.yaml\n\nIn order to edit the memsql-cluster.yaml you will need your license key from the customer portal and a hashed version of a secure password for the admin user.\n\nIn order to hash a secure password, we have an example python script:\nfrom hashlib import sha1\nprint(\"*\" + sha1(sha1('secretpass'.encode('utf-8')).digest()).hexdigest().upper())\n\nHow to use the script\n\n  Save the script in a file named hash_password.py, replacing secretpass with a secure password. Make sure it is executable with chmod +x hash_password.py, then run the script with python3 hash_password.py. It will print the hashed password to the command line, where you can copy and paste directly to the memsql-cluster.yaml file.\n\n\nCreate a namespace for the Kubernetes Objects\nkubectl create ns singlestore\n\nAdd the namespace to the object metadata\napiVersion: memsql.com/v1alpha1\nkind: MemsqlCluster\nmetadata:\n  name: memsql-cluster\n  namespace: singlestore\n\nAdd the licence_key, hashed_password keeping the quotes, and use the latest memsql/node image\nspec:\n  license: &lt;license_key&gt;\n  adminHashedPassword: \"&lt;hashed_password&gt;\"\n  nodeImage:\n    repository: memsql/node\n    tag: latest\n\nChange the redundancy level to 2\n  redundancyLevel: 2\n\nChange the storage class to default within the aggregatorSpec:\nstorageClass: default\n\n\nThe rest of the edits to the aggregatorSpec and the leafSpec are custom depending on how many aggregators and leafs you want and how many resources you want allocated to them. Save all yaml files to an accessible directory.\n\nCreate the Kubernetes Objects\n\nCreate the role-based access control\nkubectl -n singlestore create -f rbac.yaml\n\nCreate the memsql custom resource definition\nkubectl -n singlestore create -f memsql-cluster-crd.yaml\n\nCreate the Operator deployment\nkubectl -n singlestore create -f deployment.yaml\n\nNow you should be able to run kubectl -n singlestore get pods and see that the Operator pod is running.\nNAME                               READY   STATUS    RESTARTS   AGE\nmemsql-operator-79874797f4-9t47r   1/1     Running   0          2m50s\n\nCreate the memsql custom resource\nkubectl -n singlestore create -f memsql-cluster.yaml\n\nAfter a couple minutes, run kubectl -n singlestore get pods again to verify the aggregator and leaf nodes all started and have a status of Running.\n\nIf you see no pods are in the Running state, check the Operator logs by running\nkubectl -n singlestore logs deployment memsql-operator then look at the various objects to see what is failing.\n\nNow you can run kubectl -n singlestore get memsql to see the custom resource that was created.\nNAME             AGGREGATORS   LEAVES   REDUNDANCY LEVEL   AGE\nmemsql-cluster   1             1        2                  30h\n\nVerify the cluster\nkubectl get memsql memsql-cluster -o=jsonpath='{.status.phase}{\"\\n\"}' -n singlestore\n\nThe SingleStore DB server deployment is complete when Running is displayed after running the above commands.\n\nConnect to the database\nInstall the SingleStore Client\n\nAfter the deployment completes, run the following command to display the two SingleStore DB service endpoints that are created during the deployment\nkubectl -n singlestore get services\n\nThe svc-&lt;cluster-name&gt;-ddl and svc-&lt;cluster-name&gt;-dml service endpoints can be used to connect to SingleStore DB using a MySQL compatible client. Note that svc--dml only exists if  memsqlCluster.aggregatorSpec.count is greater than 1.\nNAME                     TYPE           CLUSTER-IP       EXTERNAL-IP     PORT(S)          AGE\nsvc-memsql-cluster       ClusterIP      None             &lt;none&gt;          3306/TCP         30h\nsvc-memsql-cluster-ddl   LoadBalancer   &lt;ip-address&gt;     &lt;ip-address&gt;    3306:30907/TCP   30h\n\nThe IP address under EXTERNAL-IP in the svc-memsql-cluster-ddl row is the one that you will use to connect to your database. You will use the admin user and the un-hashed password with port 3306 to connect.\n\nExample connection command\nsinglestore -h&lt;ip-address&gt; -uadmin -P3306 -p&lt;secretpass&gt;\n\nRefer to Data Definition Language DDL and Data Manipulation Language DML for more information.\n\nCheck status of aggregators and leafs\nshow aggregators;\n\nshow leaves;\n\n\nThis concludes setting up SingleStore with Azure Kubernetes Engine.\n\nDeployment Artifacts\nDeployment related files can be found at https://github.com/sdb-cloud-ops/Azure\n\n  k8s operator yamls\n  terraform scripts"
					}
					
				
		
				
					,
					
					"docs-deploying-gcp-deploying": {
						"id": "docs-deploying-gcp-deploying",
						"title": "Deploy SingleStore with Google Cloud Kubernetes Engine",
						"categories": "",
						"url": " /docs/Deploying/GCP/deploying",
						"content": "Deploy SingleStore with Google Cloud Kubernetes Engine\n\nPrerequisites\n\nMake sure you have kubectl and Google Cloud SDK installed.\n\nUse the gcloud tool to configure the following default settings: your default project, compute zone, and compute region.\n\nCreating and connecting to the Kubernetes cluster\n\nWe will use the gcloud containers create command to create our sdb-cluster using the following optional flags:\n\n[--network=NETWORK]\n[--subnetwork=SUBNETWORK]\n[--region=REGION | --zone=ZONE, -z ZONE]\n[--image-type=IMAGE_TYPE]\n[--machine-type=MACHINE_TYPE, -m MACHINE_TYPE]\n[--node-locations=ZONE,[ZONE,…]]\n[--enable-autoscaling]\n[--num-nodes NUM_NODES]\n[--min-nodes MIN_NODES]\n[--max-nodes MAX_NODES]\n\n\nExample command\ngcloud container clusters create sdb-cluster --region=us-east4 --node-locations=us-east4-a --machine-type=n2-standard-16 --image-type=cos --enable-ip-alias --create-subnetwork name=sdb-subnet --enable-autoscaling --num-nodes=4 --min-nodes=0 --max-nodes=10\n\n\n  Note: Warnings are expected in the output at this time.\n\n\nRun the gcloud container clusters get-credentials command to connect to your Kubernetes cluster.\n\nExample command\ngcloud container clusters get-credentials sdb-cluster --region us-east4 --project &lt;project-name&gt;\n\nIf you have kubectl correctly installed, you should now be able to run kubectl get ns without errors.\n\nDeploy SingleStore on Kubernetes\n\nDownload the Operator image (optional if not pulling from deployment.yaml)\n\nThe memsql/operator can be pulled from Docker Hub or from the Red Hat container registry.\n\nExample with Docker\ndocker pull memsql/operator:1.2.5-83e8133a\n\n\nCreate the Object Definition files\n\nDownload the object definition files from our site, and save them in an accessible directory. The rbac.yaml and the memsql-cluster-crd.yaml do not require edits.\n\nEdit the deployment.yaml and reference the image you downloaded, or you can pull it automatically.\n\ndeployment.yaml\n\nExample spec\n    spec:\n      serviceAccountName: memsql-operator\n      containers:\n        - name: memsql-operator\n          image: memsql/operator:1.2.5-83e8133a\n          imagePullPolicy: IfNotPresent\n\n\nmemsql-cluster.yaml\n\nIn order to edit the memsql-cluster.yaml you will need your license key from the customer portal and a hashed version of a secure password for the admin user.\n\nIn order to hash a secure password, we have an example python script:\nfrom hashlib import sha1\nprint(\"*\" + sha1(sha1('secretpass'.encode('utf-8')).digest()).hexdigest().upper())\n\nHow to use the script\n\n  Save the script in a file named hash_password.py, replacing secretpass with a secure password. Make sure it is executable with chmod +x hash_password.py, then run the script with python3 hash_password.py. It will print the hashed password to the command line, where you can copy and paste directly to the memsql-cluster.yaml file.\n\n\nCreate a namespace for the Kubernetes Objects\nkubectl create ns singlestore\n\nAdd the namespace to the object metadata\napiVersion: memsql.com/v1alpha1\nkind: MemsqlCluster\nmetadata:\n  name: memsql-cluster\n  namespace: singlestore\n\nAdd the licence_key, hashed_password keeping the quotes, and use the latest memsql/node image\nspec:\n  license: &lt;license_key&gt;\n  adminHashedPassword: \"&lt;hashed_password&gt;\"\n  nodeImage:\n    repository: memsql/node\n    tag: latest\n\nChange the redundancy level to 2\n  redundancyLevel: 2\n\nThe rest of the edits to the aggregatorSpec and the leafSpec are custom depending on how many aggregators and leafs you want and how many resources you want allocated to them. Save all yaml files to an accessible directory.\n\nCreate the Kubernetes Objects\n\nCreate the role-based access control\nkubectl -n singlestore create -f rbac.yaml\n\nCreate the memsql custom resource definition\nkubectl -n singlestore create -f memsql-cluster-crd.yaml\n\nCreate the Operator deployment\nkubectl -n singlestore create -f deployment.yaml\n\nNow you should be able to run kubectl -n singlestore get pods and see that the Operator pod is running.\nNAME                               READY   STATUS    RESTARTS   AGE\nmemsql-operator-79874797f4-9t47r   1/1     Running   0          2m50s\n\nCreate the memsql custom resource\nkubectl -n singlestore create -f memsql-cluster.yaml\n\nNow you can run kubectl -n singlestore get memsql to see the custom resource that was created.\nNAME             AGGREGATORS   LEAVES   REDUNDANCY LEVEL   AGE\nmemsql-cluster   1             1        2                  30h\n\nConnect to the database\n\nRun kubectl -n singlestore get services in order to get the ddl endpoint\nNAME                     TYPE           CLUSTER-IP       EXTERNAL-IP     PORT(S)          AGE\nsvc-memsql-cluster       ClusterIP      None             &lt;none&gt;          3306/TCP         30h\nsvc-memsql-cluster-ddl   LoadBalancer   &lt;ip-address&gt;     &lt;ip-address&gt;    3306:30907/TCP   30h\n\nThe IP address under EXTERNAL-IP in the svc-memsql-cluster-ddl row is the one that you will use to connect to your database. You will use the admin user and the un-hashed password with port 3306 to connect.\n\nExample connection command\nmysql -h&lt;ip-address&gt; -uadmin -P3306 -p&lt;secretpass&gt;\n\nThis concludes setting up SingleStore with Google Cloud Kubernetes Engine.\n\nDeployment Artifacts\nDeployment related files can be found at https://github.com/sdb-cloud-ops/GCP\n\n  k8s operator yamls\n  terraform scripts"
					}
					
				
		
				
					,
					
					"docs-deploying-deploying-home": {
						"id": "docs-deploying-deploying-home",
						"title": "SingleStore Installation",
						"categories": "",
						"url": " /docs/Deploying/deploying_home",
						"content": "Deploy SingleStore on AWS\n\nDeploy SingleStore on GCP\n\nDeploy SingleStore on Azure"
					}
					
				
		
				
					,
					
					"docs-monitoring-monitoring": {
						"id": "docs-monitoring-monitoring",
						"title": "Monitoring page",
						"categories": "",
						"url": " /docs/Monitoring/monitoring",
						"content": "Monitoring SingleStore on Kubernetes\nThis document will show you how to install Prometheus, Grafana, and Grafana Loki on a GKE cluster. It will also provide basics on Grafana use including adding the Loki data source to Grafana in order to view and query logs and exploring the default dashboards provided by the kube-prometheus stack.\n\nPrometheus\n\n  Note we are using the kube-prometheus stack. Please refer to this doc for reference.\n\n\nInstall dependencies\n\n  On MacOS you can use brew to install these packages\n\n\n\n  \n    jsonnet-bundler\n  \n  \n    jsonnet\n  \n  \n    go\n  \n  gojsontoyaml library\n      go install github.com/brancz/gojsontoyaml@latest\n  go install github.com/google/go-jsonnet/cmd/jsonnet@latest\n    \n  \n  wget\n\n\nCreate the install directory\nmkdir kube-prometheus\n\ncd kube-prometheus\n\n\nInitialize jb and install kube-prometheus\njb init\n\njb install github.com/prometheus-operator/kube-prometheus/jsonnet/kube-prometheus@main\n\n\nPull the example.jsonnet and build.sh files\nwget https://raw.githubusercontent.com/prometheus-operator/kube-prometheus/main/example.jsonnet -O example.jsonnet\n\nwget https://raw.githubusercontent.com/prometheus-operator/kube-prometheus/main/build.sh -O build.sh\n\n\nUpdate jb\njb update\n\n\nMake build.sh executable\nchmod +x build.sh\n\n\n\n  Note: If you need to update GOPATH in the build.sh file edit line as below:\n\n  jsonnet -J vendor -m manifests \"${1-example.jsonnet}\" | xargs -I{} sh -c 'cat {} | $(go env GOPATH)/bin/gojsontoyaml &gt; {}.yaml; rm -f {}' -- {}\n\n\n\nBuild the customization file\n\nvim memsql.jsonnet\n\nlocal kp = (import 'kube-prometheus/main.libsonnet') + {\n  values+:: {\n    common+: {\n      namespace: 'monitoring',\n    },\n    prometheus+:: {\n      namespaces+: ['singlestore'],\n    },\n  },\n  memsqlMetrics: {\n    serviceMonitorMyNamespace: {\n      apiVersion: 'monitoring.coreos.com/v1',\n      kind: 'ServiceMonitor',\n      metadata: {\n        name: 'memsql-metrics',\n        namespace: 'singlestore',\n      },\n      spec: {\n        endpoints: [\n          {\n            path: '/metrics',\n            targetPort: 'metrics',\n          },\n        ],\n        jobLabel: 'app.kubernetes.io/name',\n        selector: {\n          matchLabels: {\n            'app.kubernetes.io/name': 'memsql-cluster',\n            'app.kubernetes.io/component': 'cluster',\n          },\n        },\n      },\n    },\n  },\n  memsqlClusterMetrics: {\n    serviceMonitorMyNamespace: {\n      apiVersion: 'monitoring.coreos.com/v1',\n      kind: 'ServiceMonitor',\n      metadata: {\n        name: 'memsql-cluster-metrics',\n        namespace: 'singlestore',\n      },\n      spec: {\n        endpoints: [\n          {\n            path: '/cluster-metrics',\n            targetPort: 'metrics',\n          },\n        ],\n        jobLabel: 'app.kubernetes.io/name',\n        selector: {\n          matchLabels: {\n            'app.kubernetes.io/name': 'memsql-cluster',\n            'app.kubernetes.io/component': 'master',\n          },\n        },\n      },\n    },\n  },\n};\n\n{ 'setup/0namespace-namespace': kp.kubePrometheus.namespace } +\n{\n  ['setup/prometheus-operator-' + name]: kp.prometheusOperator[name]\n  for name in std.filter((function(name) name != 'serviceMonitor' &amp;&amp; name != 'prometheusRule'), std.objectFields(kp.prometheusOperator))\n} +\n// serviceMonitor and prometheusRule are separated so that they can be created after the CRDs are ready\n{ 'prometheus-operator-serviceMonitor': kp.prometheusOperator.serviceMonitor } +\n{ 'prometheus-operator-prometheusRule': kp.prometheusOperator.prometheusRule } +\n{ 'kube-prometheus-prometheusRule': kp.kubePrometheus.prometheusRule } +\n{ ['alertmanager-' + name]: kp.alertmanager[name] for name in std.objectFields(kp.alertmanager) } +\n{ ['blackbox-exporter-' + name]: kp.blackboxExporter[name] for name in std.objectFields(kp.blackboxExporter) } +\n{ ['grafana-' + name]: kp.grafana[name] for name in std.objectFields(kp.grafana) } +\n{ ['kube-state-metrics-' + name]: kp.kubeStateMetrics[name] for name in std.objectFields(kp.kubeStateMetrics) } +\n{ ['kubernetes-' + name]: kp.kubernetesControlPlane[name] for name in std.objectFields(kp.kubernetesControlPlane) }\n{ ['node-exporter-' + name]: kp.nodeExporter[name] for name in std.objectFields(kp.nodeExporter) } +\n{ ['prometheus-' + name]: kp.prometheus[name] for name in std.objectFields(kp.prometheus) } +\n{ ['prometheus-adapter-' + name]: kp.prometheusAdapter[name] for name in std.objectFields(kp.prometheusAdapter) } +\n{ ['memsql-metrics-' + name]: kp.memsqlMetrics[name] for name in std.objectFields(kp.memsqlMetrics) } +\n{ ['memsql-cluster-metrics-' + name]: kp.memsqlClusterMetrics[name] for name in std.objectFields(kp.memsqlClusterMetrics) }\n\n\n  Note: Only change the namespace inside of this file where it says singlestore to the namespace that you have SingleStore deployed.\n\n\nApply the build to the manifest files\n./build.sh memsql.jsonnet\n\n\nCreate the kubernetes objects\nkubectl apply --server-side -f manifests/setup\n\nkubectl apply -f manifests/\n\n\nTo access Prometheus locally\nRun the command below and then go to http://localhost:9090 in your web browser to access Prometheus\nkubectl --namespace monitoring port-forward svc/prometheus-k8s 9090\n\nGrafana Loki\n\n\n  Note: further installation methods are provided in the Reference section below\n\n\nGKE\n\nFor this installation, we will be creating a Google cloud storage bucket for our backend log store.\n\nPrerequisites\n\nInstall Tanka\n\n  MacOS users can use brew install tanka\n\n\nCreate a Google cloud bucket and service account\nGo to the Google cloud storage browser, and click create a bucket. We will need to create a service account in order for Loki to access the bucket, so next, go to IAM &amp; Admin &gt; Service Accounts, and create a service account. I gave my service account Storage Admin and Storage Object Admin roles. Once the service account is created, click into the account and go to the Keys section. There you can click add key and make sure to save the downloaded file.\n\nCreate a secret based off the google key\nkubectl -n monitoring create secret generic google-key --from-file=key.json=&lt;path/to/downloaded/key.json&gt;\n\n\nInstall Grafana Loki with Tanka\n\n  Please refer to this documentation for reference.\n\n\nCreate a local directory for the Loki environment\nmkdir loki\n\ncd loki\n\nInitialize Tanka\ntk init\n\nFind your kubernetes api server\nvim ~/.kube/config\n\nFind the - cluster section of the cluster you are using, and locate the server: section. It will be right below the cluster certificate, and right above the cluster name. This is your kubernetes api server.\ntk env add environments/loki --namespace=monitoring --server=&lt;Kubernetes API server&gt;\n\nDownload and install the Loki and Promtail module using jb\njb install github.com/grafana/loki/production/ksonnet/loki@main\njb install github.com/grafana/loki/production/ksonnet/promtail@main\n\nCreate the htpasswd file\nThis example creates a .loki file with the loki username\nhtpasswd -c .loki loki\n\nIt will prompt for a new password. Enter this twice. Then view the contents of the file for input into the htpasswd_contents: section of the main.jsonnet\n\nEdit the environments/loki/main.jsonnet file\nMake sure to replace the commented items with values that are specific to your deployment.\nvim environments/loki/main.jsonnet\n\nlocal gateway = import 'loki/gateway.libsonnet';\nlocal loki = import 'loki/loki.libsonnet';\nlocal promtail = import 'promtail/promtail.libsonnet';\nlocal k = import 'ksonnet-util/kausal.libsonnet';\nlocal gcsBucket = import 'gcsBucket.libsonnet';\n\nloki + promtail + gateway + gcsBucket {\n\n  _config+:: {\n    namespace: 'loki',\n    htpasswd_contents: 'loki:$apr1$i.G5yqP8$LGo1ANcwfGH87unfpIr6m.', //content of your .loki file created above\n\n    // GCS variables -- Remove if not using gcs\n    storage_backend: 'gcs',\n    gcs_bucket_name: 'sdb-loki', //name of the google bucket\n\n    //Set this variable based on the type of object storage you're using.\n    boltdb_shipper_shared_store: 'gcs',\n\n    //Update the object_store and from fields\n    loki+: {\n      schema_config: {\n        configs: [{\n          from: '2022-02-02', //set this date to the date exactly 2 weeks ago from today\n          store: 'boltdb-shipper',\n          object_store: 'gcs',\n          schema: 'v11',\n          index: {\n            prefix: '%s_index_' % $._config.table_prefix,\n            period: '%dh' % $._config.index_period_hours,\n          },\n        }],\n      },\n    },\n\n    //Update the container_root_path if necessary\n    promtail_config+: {\n      clients: [{\n        scheme:: 'http',\n        hostname:: 'gateway.%(namespace)s.svc' % $._config,\n        username:: 'loki',\n        password:: 'loki', //same password that was used for the loki username in the htpasswd file\n        container_root_path:: '/var/lib/docker',\n      }],\n    },\n\n    replication_factor: 3,\n    consul_replicas: 1,\n  },\n}\n\nCreate a new file in the lib directory that edits the yaml files for GKE specific variables\nvim lib/gcsBucket.libsonnet\n\n//gcsBucket.libsonnet\n\n// Import libs\n{\n  local volumeMount = $.core.v1.volumeMount,\n  local container = $.core.v1.container,\n  local statefulSet = $.apps.v1.statefulSet,\n  local volume = $.core.v1.volume,\n  local secret = $.core.v1.secret,\n  local deployment = $.apps.v1.deployment,\n  local pvc = $.core.v1.persistentVolumeClaim,\n  local spec = $.core.v1.spec,\n  local storageClass = $.storage.v1.storageClass,\n\n  //distributor\n  distributor_container+::\n    container.withEnvMap({GOOGLE_APPLICATION_CREDENTIALS:'/var/secrets/google/key.json'}),\n\n  distributor_deployment+:\n    $.util.secretVolumeMount('google', '/var/secrets/google'),\n\n  distributor_args+:: {\n    'config.expand-env':'true',\n  },\n\n\n  //querier\n  querier_container+::\n    container.withEnvMap({GOOGLE_APPLICATION_CREDENTIALS:'/var/secrets/google/key.json'}),\n\n  querier_statefulset+::\n    $.util.secretVolumeMount('google', '/var/secrets/google'),\n\n  querier_args+:: {\n    'config.expand-env':'true',\n  },\n\n  querier_data_pvc+:\n    pvc.mixin.spec.withStorageClassName('standard'),\n\n\n  //ingester\n  ingester_container+::\n    container.withEnvMap({GOOGLE_APPLICATION_CREDENTIALS:'/var/secrets/google/key.json'}),\n\n  ingester_statefulset+:\n    $.util.secretVolumeMount('google', '/var/secrets/google'),\n\n  ingester_args+:: {\n    'config.expand-env':'true',\n  },\n\n  ingester_data_pvc+::\n    pvc.mixin.spec.withStorageClassName('standard'),\n\n  ingester_wal_pvc+::\n    pvc.mixin.spec.withStorageClassName('standard'),\n\n  //compactor\n  compactor_container+::\n    container.withEnvMap({GOOGLE_APPLICATION_CREDENTIALS:'/var/secrets/google/key.json'}),\n\n  compactor_statefulset+:\n    $.util.secretVolumeMount('google', '/var/secrets/google'),\n\n  compactor_args+:: {\n    'config.expand-env':'true',\n  },\n\n  compactor_data_pvc+::\n    pvc.mixin.spec.withStorageClassName('standard'),\n\n\n  //table-manager\n  table_manager_container+::\n    container.withEnvMap({GOOGLE_APPLICATION_CREDENTIALS:'/var/secrets/google/key.json'}),\n\n  table_manager_deployment+:\n    $.util.secretVolumeMount('google', '/var/secrets/google'),\n\n  table_manager_args+:: {\n    'config.expand-env':'true',\n  },\n\n\n  //query-frontend\n  query_frontend_container+::\n    container.withEnvMap({GOOGLE_APPLICATION_CREDENTIALS:'/var/secrets/google/key.json'}),\n\n  query_frontend_deployment+:\n    $.util.secretVolumeMount('google', '/var/secrets/google'),\n\n  query_frontend_args+:: {\n    'config.expand-env':'true',\n  },\n\n}\n\nUse tanka to create the yaml files\ntk export manifests environments/loki\n\nThis will give you a manifests directory of yaml files for review.\n\nApply the yaml files to the kubernetes cluster\nkubectl --namespace monitoring create -f manifests\n\n\nAdd the Loki data source to Grafana\n\nRun the command below and then go to http://localhost:3000 in your web browser to access Grafana\nkubectl --namespace monitoring port-forward svc/grafana 3000:3000\n\n\nGo to Configuration &gt; Data Sources\n\n\nClick add data source and then type in ‘Loki’\n\n\nEnter the Loki URL\n\n\nClick ‘Save &amp; Test’ and you should see the message below\n\n\nNow you can explore SingleStore logs through Grafana + Loki\n\n\nExpand the Log Browser in order to have a helpful UI\n\n\nExample query with SingleStore logs\n\n\n\n  Here is further information on LogQL Queries\n\n\nAccess Grafana Dashboards\n\n  Kube-prometheus added 24 dashboards to Grafana to get you started with dashboard monitoring\n\n\nClick on Dashboards &gt; Browse\n\n\nExpand the Default folder\n\n\nClick on a dashboard to try it out!\n\nNow we have installed a monitoring stack consisting of Prometheus and Grafana Loki. Further reading:\nDashboard overview,\nGrafana alerting\n\nReference\n\nFurther installation methods of Grafana Loki\n\nInstall Grafana Loki with Helm with a persistant volume claim\n\n\n  Further installation methods are detailed here\n\n\nPrereqs: Install Helm, then run the commands below\n\nhelm repo add grafana https://grafana.github.io/helm-charts\n\nhelm repo update\n\n\nInstall Loki + Promtail\nhelm upgrade --install loki --namespace=monitoring grafana/loki-stack  --set loki.persistence.enabled=true,loki.persistence.storageClassName=standard,loki.persistence.size=5Gi\n\n\n  Note: you can change the size of the volume claim\n\n\nGrafana is accessed the same way, but not that the Loki URL is http://loki:3100 when adding the Loki datasource to Grafana after installing with Helm"
					}
					
				
		
				
					,
					
					"docs-scaling-scaling": {
						"id": "docs-scaling-scaling",
						"title": "Scaling page",
						"categories": "",
						"url": " /docs/Scaling/scaling",
						"content": "Scaling\n\nWe need to modify the resource only. All jobs are done by operators. You\nneed to monitor the log of the pod operator during scaling to identify\nif the scaling is in progress, done or has a problem.\n\nPrerequisites\n\n\n  \n    Ensure the backup operator is running and the backup file exists.\n  \n  \n    Get the memsql cluster\n\n    kubectl get memsqlcluster -n&lt;namespace&gt;\n    \n  \n  \n    Check current spec of MemSQL cluster\n\n    kubectl get memsqlcluster memsql-cluster -n&lt;namespace&gt; -o yaml\n    \n  \n  \n    Definition\n\n    count: number of nodes\n\nheight: cpu &amp; memory limit\n\nstorageGB: storage size\n    \n  \n  \n    Monitor log of operator\n\n    Open a new terminal window.\n\n    kubectl get po -n&lt;namespace&gt;\n\n\nkubectl logs memsql-operator-&lt;x&gt; -n&lt;namespace&gt; -f\n    \n  \n  \n    Monitor the pod during scaling\n\n    Open a new terminal window\n\n    watch kubectl get po -n&lt;namespace&gt;\n    \n  \n\n\nHorizontal Scaling\n\nIncreasing the number of cluster nodes. Adding an aggregator or leaf\nnodes.\n\n\n  \n    Create scaling patch file:\n\n    cat &gt; scaling.yaml &lt;&lt;EOF\nspec:\n  aggregatorSpec:\n    count: 1\n  leafSpec:\n    count: 2\nEOF\n    \n  \n  \n    Apply your changes on memsql cluster by patching the object using patching file\n\n    kubectl patch memsqlcluster memsql-cluster --type merge --patch-file scaling.yaml -n&lt;namespace&gt;\n    \n  \n  \n    Verification\n\n    New leaves pod created\n\n    kubectl get po -n&lt;namespace&gt;\n    \n  \n\n\nVertical Scaling\n\nIncreasing node’s resources, such as cpu, memory or storage.\n\nCPU &amp; Memory\n\nRequirement:\n\nThe number of cpu &amp; memory defined in args of operator should be greater\nthan zero\n\n  kubectl get deployment memsql-operator -n&lt;namespace&gt; -oyaml\n\n\nNote: You can do vertical scaling in the current cluster if and only if the number of cores-per-unit or memory-per-unit is greater than zero else you need to recreate a new cluster then reattach the storage.\n\nWe have two ways to do vertical scaling\n\nOperator\n\nModify the operator configuration\n\n\n  \n    Edit the memsql-operator deployment\n\n    kubectl edit deploy memsql-operator -n&lt;namespace&gt;\n    \n  \n  \n    Change the number of cores-per-unit or memory-per-unit. Ensure the number greater than zero.\n  \n  \n    Save the changes\n  \n\n\nResult:\n\n\n  \n    Old pod operator will destroy\n  \n  \n    New pod operator will created\n  \n  \n    Monitor log of new operator\n\n    kubectl logs memsql-operator-&lt;x&gt; -n&lt;namespace&gt; -f\n    \n  \n\n\nCluster\n\nModify the node’s spec\n\n\n  \n    Create scaling patch file\n\n    cat &gt; scaling.yaml &lt;&lt;EOF\nspec:\n  aggregatorSpec:\n    height: 0.125\n  leafSpec:\n    height: 0.25\nEOF\n    \n  \n  \n    Get the pods\n\n    kubectl get po -n&lt;namespace&gt;|grep node\n    \n\n    Sample output:\n  \n  \n    Check the resource before changes\n\n    kubectl get pods &lt;pod-name&gt; -n&lt;namespace&gt; -o jsonpath='{range .spec.containers[?(@.name==\"node\")]}{\"Container Name:\"}{.name}{\"\\n\"}{\"Requests:\"}{.resources.requests}{\"\\n\"}{\"Limits:\"}{.resources.limits}{\"\\n\"}{end}'\n    \n\n    Sample output:\n  \n  \n    Apply your changes on memsql cluster by patching the object using patching file\n\n    kubectl patch memsqlcluster memsql-cluster --type merge --patch-file scaling.yaml -n&lt;namespace&gt;\n    \n  \n  \n    Verification\n\n    Verify the number of cpu &amp; memory is increase as per desired\n\n    kubectl get pod &lt;pod-name&gt; -n&lt;namespace&gt; -o jsonpath='{range .spec.containers[?(@.name==\"node\")]}{\"Container Name: \"}{.name}{\"\\n\"}{\"Requests:\"}{.resources.requests}{\"\\n\"}{\"Limits:\"}{.resources.limits}{\"\\n\"}{end}'\n    \n  \n\n\nStorage\n\n\n  \n    Requirement\n\n    a.  The storage class should be allow volume expansion\n\n    kubectl get sc &lt;sc-name&gt; -oyaml\n    \n\n    b.  Support modifying a Disk Size to a larger size only\n  \n  \n    Check storage\n\n    Check the capacity of pvc/pv.\n\n    kubectl get pvc -n&lt;namespace&gt;\n\nkubectl get pv\n    \n  \n  \n    Create scaling patch file\n\n    cat &gt; scaling.yaml &lt;&lt;EOF\nspec:\n  aggregatorSpec:\n    storageGB: 20\n  leafSpec:\n    storageGB: 25\nEOF\n    \n  \n  \n    Apply your changes on memsql cluster by patching the object using patching file\n\n    kubectl patch memsqlcluster memsql-cluster --type merge --patch-file scaling.yaml -n&lt;namespace&gt;\n    \n  \n  \n    Verify\n\n    Check the capacity of pvc/pv.\n\n    kubectl get pvc -n&lt;namespace&gt;\n\nkubectl get pv\n    \n  \n\n\nTroubleshoot\n\n\n  \n    Error during resize the storage\n\n    kubectl describe pvc  -n\n\n    Error message in pod operator:\n\n    errors.go:92 {controller.memsql} Reconciler error will retry after: \"55s\" error: \"Waiting for PVC pv-storage-node-memsql-cluster-leaf-ag1-0 to finish controller resizing\"\n    \n\n    Root cause:\n\n    The storage provisioner required detach the pod before resizing the volume\n\n    Solution:\n\n    \n      \n        Delete the pod\n\n        kubectl delete po  -n\n      \n      \n        Downsizing the replicas on related statefulset if option 1 doesn’t effect:\n\n        kubectl -n patch sts  --type merge -p '{\"spec\":{\"replicas\": 0}}'\n\n        Result:\n\n        \n          \n            The pod will be deleted\n          \n          \n            PVC will be resized\n          \n          \n            The new pod will recreate"
					}
					
				
		
		
				
					,
					
					"2019-hello-world": {
						"id": "2019-hello-world",
						"title": "Two Thousand Nineteen",
						"categories": "jekyll, update",
						"url": " /2019/hello-world/",
						"content": "Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum."
					}
					
				
		
				
					,
					
					"2019-welcome": {
						"id": "2019-welcome",
						"title": "Welcome to Docsy Jekyll",
						"categories": "jekyll, update",
						"url": " /2019/welcome/",
						"content": "You’ll find this post in your _posts directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run jekyll serve, which launches a web server and auto-regenerates your site when a file is updated.\n\n\n\nTo add new posts, simply add a file in the _posts directory that follows the convention YYYY-MM-DD-name-of-post.ext and includes the necessary front matter. Take a look at the source for this post to get an idea about how it works.\n\nJekyll also offers powerful support for code snippets:\n\ndef print_hi(name)\n  puts \"Hi, #{name}\"\nend\nprint_hi('Tom')\n#=&gt; prints 'Hi, Tom' to STDOUT.\n\nCheck out the Jekyll docs for more info on how to get the most out of Jekyll. File all bugs/feature requests at Jekyll’s GitHub repo. If you have questions, you can ask them on Jekyll Talk."
					}
					
				
		
		
				
					,
					
					"404-html": {
						"id": "404-html",
						"title": "",
						"categories": "",
						"url": " /404.html",
						"content": "404\n\n  Page not found :(\n  The requested page could not be found."
					}
					
				
		
				
					,
					
					"about": {
						"id": "about",
						"title": "About",
						"categories": "",
						"url": " /about/",
						"content": "This is the base Jekyll theme. You can find out more info about customizing your Jekyll theme, as well as basic Jekyll usage documentation at jekyllrb.com\n\nYou can find the source code for Minima at GitHub:\njekyll /\nminima\n\nYou can find the source code for Jekyll at GitHub:\njekyll /\njekyll"
					}
					
				
		
				
					,
					
					"about": {
						"id": "about",
						"title": "About",
						"categories": "",
						"url": " /about/",
						"content": "About\n\nThis is a starter template for a Docsy jekyll theme, based\non the Beautiful Docsy that renders with Hugo. This version is intended for\nnative deployment on GitHub pages. See the respository for more details.\n\nSupport\n\nIf you need help, please don’t hesitate to open an issue."
					}
					
				
		
				
					,
					
					"archive": {
						"id": "archive",
						"title": "Articles",
						"categories": "",
						"url": " /archive/",
						"content": "News Archive\n\n2019\n\n\n  Jun 29, 2019: Two Thousand Nineteen\n  \n\n\n\n  Jun 28, 2019: Welcome to Docsy Jekyll"
					}
					
				
		
				
					,
					
					"docs": {
						"id": "docs",
						"title": "Technical Guides",
						"categories": "",
						"url": " /docs/",
						"content": "Technical Guides\n\nWelcome to the Cloud-ops Documentation pages! Here you can quickly jump to a \nparticular page.\n\n\n    \n            \n    \n    Deploy SingleStore with Amazon Web Service\n    \n            \n    \n    Deploy SingleStore with Azure Kubernetes Service\n    \n            \n    \n    Deploy SingleStore with Google Cloud Kubernetes Engine\n    \n            \n    \n    SingleStore Installation\n    Installing SingleStore on public cloud providers\n            \n    \n    Monitoring page\n    \n            \n    \n    Scaling page"
					}
					
				
		
				
					,
					
					"feed-xml": {
						"id": "feed-xml",
						"title": "",
						"categories": "",
						"url": " /feed.xml",
						"content": "Cloud-ops\n    \n    http://localhost:4000/sdb-cloud-ops.github.io/\n    \n    Wed, 11 May 2022 16:48:07 +0530\n    Wed, 11 May 2022 16:48:07 +0530\n    Jekyll v3.9.0\n    \n      \n        Two Thousand Nineteen\n        &lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.&lt;/p&gt;\n\n        Sat, 29 Jun 2019 00:22:21 +0530\n        http://localhost:4000/sdb-cloud-ops.github.io/2019/hello-world/\n        http://localhost:4000/sdb-cloud-ops.github.io/2019/hello-world/\n        \n        \n        jekyll\n        \n        update\n        \n      \n    \n      \n        Welcome to Docsy Jekyll\n        &lt;p&gt;You’ll find this post in your &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_posts&lt;/code&gt; directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;jekyll serve&lt;/code&gt;, which launches a web server and auto-regenerates your site when a file is updated.&lt;/p&gt;\n\n&lt;!--more--&gt;\n\n&lt;p&gt;To add new posts, simply add a file in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_posts&lt;/code&gt; directory that follows the convention &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;YYYY-MM-DD-name-of-post.ext&lt;/code&gt; and includes the necessary front matter. Take a look at the source for this post to get an idea about how it works.&lt;/p&gt;\n\n&lt;p&gt;Jekyll also offers powerful support for code snippets:&lt;/p&gt;\n\n&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-ruby&quot; data-lang=&quot;ruby&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;print_hi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;\n  &lt;span class=&quot;nb&quot;&gt;puts&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;Hi, &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;#{&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;\n&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;\n&lt;span class=&quot;n&quot;&gt;print_hi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&apos;Tom&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;\n&lt;span class=&quot;c1&quot;&gt;#=&amp;gt; prints &apos;Hi, Tom&apos; to STDOUT.&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;\n\n&lt;p&gt;Check out the &lt;a href=&quot;http://jekyllrb.com/docs/home&quot;&gt;Jekyll docs&lt;/a&gt; for more info on how to get the most out of Jekyll. File all bugs/feature requests at &lt;a href=&quot;https://github.com/jekyll/jekyll&quot;&gt;Jekyll’s GitHub repo&lt;/a&gt;. If you have questions, you can ask them on &lt;a href=&quot;https://talk.jekyllrb.com/&quot;&gt;Jekyll Talk&lt;/a&gt;.&lt;/p&gt;\n\n\n        Fri, 28 Jun 2019 16:22:21 +0530\n        http://localhost:4000/sdb-cloud-ops.github.io/2019/welcome/\n        http://localhost:4000/sdb-cloud-ops.github.io/2019/welcome/\n        \n        \n        jekyll\n        \n        update"
					}
					
				
		
				
					,
					
					"": {
						"id": "",
						"title": "",
						"categories": "",
						"url": " /",
						"content": ""
					}
					
				
		
				
					,
					
					"": {
						"id": "",
						"title": "CloudOps Documentation",
						"categories": "",
						"url": " /",
						"content": "Welcome to Cloud Operations documentation pages\n\nThis documentation contains the best practices and guidance to the SingelStore Field team on how to stand up production capable environments for CSP platforms and customer on-prem infrastructure.\n\nPlease use the left menu to navigate through the site.\n\nFor quick searching, use the search bar on the left or use the tags page"
					}
					
				
		
				
		
				
					,
					
					"assets-js-main-js": {
						"id": "assets-js-main-js",
						"title": "",
						"categories": "",
						"url": " /assets/js/main.js",
						"content": "(function($) {\n    'use strict';\n    $(function() {\n        $('[data-toggle=\"tooltip\"]').tooltip();\n        $('[data-toggle=\"popover\"]').popover();\n        $('.popover-dismiss').popover({\n            trigger: 'focus'\n        })\n    });\n\n    function bottomPos(element) {\n        return element.offset().top + element.outerHeight();\n    }\n    $(function() {\n        var promo = $(\".js-td-cover\");\n        if (!promo.length) {\n            return\n        }\n        var promoOffset = bottomPos(promo);\n        var navbarOffset = $('.js-navbar-scroll').offset().top;\n        var threshold = Math.ceil($('.js-navbar-scroll').outerHeight());\n        if ((promoOffset - navbarOffset) < threshold) {\n            $('.js-navbar-scroll').addClass('navbar-bg-onscroll');\n        }\n        $(window).on('scroll', function() {\n            var navtop = $('.js-navbar-scroll').offset().top - $(window).scrollTop();\n            var promoOffset = bottomPos($('.js-td-cover'));\n            var navbarOffset = $('.js-navbar-scroll').offset().top;\n            if ((promoOffset - navbarOffset) < threshold) {\n                $('.js-navbar-scroll').addClass('navbar-bg-onscroll');\n            } else {\n                $('.js-navbar-scroll').removeClass('navbar-bg-onscroll');\n                $('.js-navbar-scroll').addClass('navbar-bg-onscroll--fade');\n            }\n        });\n    });\n}(jQuery));\n(function($) {\n    'use strict';\n    var Search = {\n        init: function() {\n            $(document).ready(function() {\n                $(document).on('keypress', '.td-search-input', function(e) {\n                    if (e.keyCode !== 13) {\n                        return\n                    }\n                    var query = $(this).val();\n                    var searchPage = \"http://localhost:4000/sdb-cloud-ops.github.io/search/?q=\" + query;\n                    document.location = searchPage;\n                    return false;\n                });\n            });\n        },\n    };\n    Search.init();\n}(jQuery));"
					}
					
				
		
				
					,
					
					"news": {
						"id": "news",
						"title": "News",
						"categories": "",
						"url": " /news/",
						"content": "News\n\nSubscribe with RSS to keep up with the latest news.\nFor site changes, see the changelog kept with the code base.\n\n\n\n\n   Two Thousand Nineteen\n   June 29, 2019\n   warning-badgedanger-badge\n   Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n\n   \n   \n\n\n\n   Welcome to Docsy Jekyll\n   June 28, 2019\n   primary-badgesecondary-badgeinfo-badgesuccess-badge\n   You’ll find this post in your _posts directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run jekyll serve, which launches a web server and auto-regenerates your site when a file is updated.\n\n\n   \n      read more\n   \n   \n\n\nWant to see more? See the News Archive."
					}
					
				
		
				
		
				
		
				
		
				
					,
					
					"sitemap-xml": {
						"id": "sitemap-xml",
						"title": "",
						"categories": "",
						"url": " /sitemap.xml",
						"content": "/\n     {{ \"now\" | date: \"%Y-%m-%d\" }}\n     daily\n    \n{% for section in site.data.toc %}\n     {{ site.baseurl }}{{ section.url }}/\n     {{ \"now\" | date: \"%Y-%m-%d\" }}\n     daily\n    \n{% endfor %}"
					}
					
				
		
				
					,
					
					"tags": {
						"id": "tags",
						"title": "Tags Index",
						"categories": "",
						"url": " /tags/",
						"content": "Tags Index\n{% capture site_tags %}{% for tag in site.tags %}{% if tag %}{{ tag | first }}{% unless forloop.last %},{% endunless %}{% endif %}{% endfor %}{% endcapture %}{% assign docs_tags = \"\" %}{% for doc in site.docs %}{% assign ttags = doc.tags | join:',' | append:',' %}{% assign docs_tags = docs_tags | append:ttags %}{% endfor %}\n{% assign all_tags = site_tags | append:docs_tags %}{% assign tags_list = all_tags | split:',' | uniq | sort %}\n\n{% for tag in tags_list %}{% if tag %}{{ tag }}\n\n    {% for post in site.tags[tag] %}\n    {{- post.title -}}\n     {{- post.date | date: \"%B %d, %Y\" -}}\n{%- endfor -%}\n{% for doc in site.docs %}{% if doc.tags contains tag %}\n\n    {{ doc.title }}\n         {{- doc.date | date: \"%B %d, %Y\" -}}\n    {% endif %}{% endfor %}\n{% endif %}{%- endfor -%}"
					}
					
				
		
				
					,
					
					"assets-css-style-css": {
						"id": "assets-css-style-css",
						"title": "",
						"categories": "",
						"url": " /assets/css/style.css",
						"content": "@import \"jekyll-theme-primer\";"
					}
					
				
		
	};
</script>
<script src="/sdb-cloud-ops.github.io/assets/js/lunr.min.js"></script>
<script src="/sdb-cloud-ops.github.io/assets/js/search.js"></script>

<script
  src="/sdb-cloud-ops.github.io/assets/js/jquery-3.3.1/jquery-3.3.1.min.js"
  integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8="
  crossorigin="anonymous"></script>

<script>
$(document).ready(function() {

    var toc = $('#TOC');

    // Select each header
    sections = $('.td-content h1');
        $.each(sections, function(idx, v) {
            section = $(v);
            var div_id = $(section).attr('id');
            var div_text = section.text().split('¶')[0];
            var parent = $("#" + div_id)
            var content = '<li id="link_' + div_id + '" class="md-nav__item"><a class="md-nav__link" href="#' + div_id + '" title="' + div_text +'">' + div_text +'</a></li>';
            $(toc).append(content);

            // Add section code to subnavigation
            var children = $('<nav class="md-nav"><ul class="md-nav__list"></nav></ul>')
            var contenders = $("#" + div_id).nextUntil("h1");
            $.each(contenders, function(idx, contender){
               if($(contender).is('h2') || $(contender).is('h3')) {
                   var contender_id = $(contender).attr('id');
                   var contender_text = $(contender).text().split('¶')[0];
                   var content = '<li class="md-nav__item"><a class="md-nav__link" href="#' + contender_id + '" title="' + contender_text +'">' + contender_text +'</a></li>';
                   children.append(content);
                }
             })
             $("#link_" + div_id).append(children);
        });
    });
</script>

<script>
var headers = ["h1", "h2", "h3", "h4"]
var colors = ["red", "orange", "green", "blue"]

$.each(headers, function(i, header){
    var color = colors[i];
    $(header).each(function () {
        var href=$(this).attr("id");
        $(this).append('<a class="headerlink" style="color:' + color + '" href="#' + href + '" title="Permanent link">¶</a>')
    });
})
</script>




              
              <!--<style>
  .feedback--answer {
    display: inline-block;
  }
  .feedback--answer-no {
    margin-left: 1em;
  }
  .feedback--response {
    display: none;
    margin-top: 1em;
  }
  .feedback--response__visible {
    display: block;
  }
</style>
<h5 class="feedback--title">Feedback</h5>
<p class="feedback--question">Was this page helpful?</p>
<button class="feedback--answer feedback--answer-yes">Yes</button>
<button class="feedback--answer feedback--answer-no">No</button>
<p class="feedback--response feedback--response-yes">
  Glad to hear it! Please <a href="https://memsql.atlassian.net/secure/CreateIssueDetails!init.jspa?pid=11934&issuetype=10005&summary=Title%20goes%20here&description=Description%20goes%20here&priority=3">tell us how we can improve</a>.
</p>
<p class="feedback--response feedback--response-no">
  Sorry to hear that. Please <a href="https://memsql.atlassian.net/secure/CreateIssueDetails!init.jspa?pid=11934&issuetype=10005&summary=Title%20goes%20here&description=Description%20goes%20here&priority=3">tell us how we can improve</a>.
</p>
<script>
  const yesButton = document.querySelector('.feedback--answer-yes');
  const noButton = document.querySelector('.feedback--answer-no');
  const yesResponse = document.querySelector('.feedback--response-yes');
  const noResponse = document.querySelector('.feedback--response-no');
  const disableButtons = () => {
    yesButton.disabled = true;
    noButton.disabled = true;
  };
  const sendFeedback = (value) => {
    if (typeof ga !== 'function') return;
    const args = {
      command: 'send',
      hitType: 'event',
      category: 'Helpful',
      action: 'click',
      label: window.location.pathname,
      value: value
    };
    ga(args.command, args.hitType, args.category, args.action, args.label, args.value);
  };
  yesButton.addEventListener('click', () => {
    yesResponse.classList.add('feedback--response__visible');
    disableButtons();
    sendFeedback(1);
  });
  noButton.addEventListener('click', () => {
    noResponse.classList.add('feedback--response__visible');
    disableButtons();
    sendFeedback(0);
  });
</script><br/>
-->
           </div>
          </main>
        </div>
      </div>
      <!--<footer class="bg-dark py-5 row d-print-none">
  <div class="container-fluid mx-sm-5">
    <div class="row">
      <div class="col-6 col-sm-4 text-xs-center order-sm-2">
        <ul class="list-inline mb-0">  
          
            <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="" aria-label="Twitter" data-original-title="Twitter">
              <a class="text-white" target="_blank" href="https://twitter.com/vsoch">
                <i class="fab fa-twitter"></i>
              </a>
            </li>
          
          
            <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="" aria-label="LinkedIn" data-original-title="LinkedIn">
              <a class="text-white" target="_blank" href="https://linkedin.com/in/vsochat">
                <i class="fab fa-linkedin"></i>
              </a>
            </li>
          
        </ul>
      </div>
      <div class="col-6 col-sm-4 text-right text-xs-center order-sm-3">
        <ul class="list-inline mb-0">  
          <li class="list-inline-item mx-2 h3" data-toggle="tooltip" data-placement="top" title="" aria-label="GitHub" data-original-title="GitHub">
            <a class="text-white" target="_blank" href="https://github.com/vsoch/docsy-jekyll">
              <i class="fab fa-github"></i>
            </a>
          </li>
        </ul>
      </div>
      <div class="col-12 col-sm-4 text-center py-2 order-sm-2">
        <small class="text-white">© 2022 Dinosaur Avocado All Rights Reserved</small>
        
        <p class="mt-2"><a href="/about/">About Docsy</a></p>	
      </div>
    </div>
  </div>
</footer>
-->
    </div>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.3/umd/popper.min.js" integrity="sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/js/bootstrap.min.js" integrity="sha384-ChfqqxuZUCnJSK3+MXmPNIyE6ZbWh2IMqE241rYiqJxyMiZ6OW/JmZQ5stwEULTy" crossorigin="anonymous"></script>
<script src="/sdb-cloud-ops.github.io/assets/js/main.js"></script>

  </body>
</html>

